+++
title = "Laboratoire pratique"
weight = 3
+++

# Laboratoire : Chain of Thought avec Ollama

## Objectifs p√©dagogiques

Ce laboratoire permet d'exp√©rimenter concr√®tement les techniques de Chain of Thought (CoT) √† l'aide d'un mod√®le de langage local. √Ä la fin de ce laboratoire, vous serez capable de :

1. Configurer et utiliser Ollama pour ex√©cuter des LLMs localement
2. Comparer les performances d'un mod√®le avec et sans Chain of Thought
3. Impl√©menter diff√©rentes variantes de CoT (Zero-Shot, Few-Shot)
4. Mesurer l'impact du CoT sur la pr√©cision et le temps de r√©ponse
5. Cr√©er des prompts CoT adapt√©s √† diff√©rents types de probl√®mes

**Dur√©e estim√©e** : 2-3 heures  
**Niveau** : Interm√©diaire  
**Pr√©requis** : Notions de base en IA et prompt engineering

---

---

## Partie 1 : Installation et configuration

### 1.1 Installation d'Ollama

**Windows** :
```powershell
# T√©l√©charger depuis https://ollama.com/download
# Ou avec winget
winget install Ollama.Ollama
```

**macOS** :
```bash
brew install ollama
```

**Linux** :
```bash
curl -fsSL https://ollama.com/install.sh | sh
```

**V√©rification de l'installation** :
```powershell
ollama --version
```

R√©sultat attendu : `ollama version 0.x.x`

### 1.2 T√©l√©chargement des mod√®les

**Mod√®le recommand√©** : Llama 3.2 (3B param√®tres, ~2GB)

```powershell
ollama pull llama3.2
```

**Mod√®les alternatifs** :

| Mod√®le | Taille | Param√®tres | Caract√©ristiques |
|--------|--------|------------|-------------------|
| `llama3.2` | 2GB | 3B | Rapide, id√©al pour d√©monstration |
| `phi3` | 2.3GB | 3.8B | √âquilibr√©, bon raisonnement |
| `mistral` | 4.1GB | 7B | Performance sup√©rieure |
| `llama3.1` | 4.7GB | 8B | Excellent pour CoT complexe |

### 1.3 Test de fonctionnement

**Mode interactif** :
```powershell
ollama run llama3.2
```

Testez avec :
```
>>> Quelle est la capitale de la France ?
>>> /bye
```

**Ligne de commande** :
```powershell
ollama run llama3.2 "Explique bri√®vement ce qu'est un LLM."
```

**Commandes utiles** :
```powershell
# Lister les mod√®les
ollama list

# Informations sur un mod√®le
ollama show llama3.2

# Supprimer un mod√®le
ollama rm nom_du_modele
```

### 1.4 Configuration Python (optionnel)

Pour automatiser les tests :

```powershell
# Cr√©er un environnement virtuel
python -m venv venv
.\venv\Scripts\Activate

# Installer les d√©pendances
pip install requests
```

---

## Partie 2 : Exp√©rimentations comparatives

### M√©thodologie

Pour chaque exercice, nous comparerons syst√©matiquement :
1. La r√©ponse **sans Chain of Thought**
2. La r√©ponse **avec Chain of Thought**
3. La pr√©cision, le temps de r√©ponse et la qualit√© du raisonnement

---

### Exercice 1 : Probl√®me arithm√©tique simple

### Sans Chain of Thought

**Commande** :
```powershell
ollama run llama3.2 "Un magasin a 23 pommes. Il re√ßoit 45 pommes le matin, puis vend 18 pommes l'apr√®s-midi. Combien reste-t-il de pommes ?"
```

**R√©sultat attendu** :
> "Il reste 50 pommes."

**Probl√®me** : Pas de justification, impossible de v√©rifier.

---

### Avec Chain of Thought

**Commande** :
```powershell
ollama run llama3.2 "Un magasin a 23 pommes. Il re√ßoit 45 pommes le matin, puis vend 18 pommes l'apr√®s-midi. Combien reste-t-il de pommes ? R√©sous ce probl√®me √©tape par √©tape."
```

**R√©sultat attendu** :
```
√âtape 1 : D√©but = 23 pommes
√âtape 2 : Apr√®s r√©ception = 23 + 45 = 68 pommes
√âtape 3 : Apr√®s vente = 68 - 18 = 50 pommes
R√©ponse finale : 50 pommes
```

**‚úÖ Avantage** : Raisonnement transparent et v√©rifiable.

---

## üß™ D√©monstration 2 : Probl√®me avec pourcentages

### Sans CoT

**Commande** :
```powershell
ollama run llama3.2 "Un produit co√ªte 120‚Ç¨. On applique une r√©duction de 20%, puis une taxe de 10% sur le prix r√©duit. Quel est le prix final ?"
```

**Observation** : R√©ponse souvent incorrecte ou confuse.

---

### Avec CoT

**Commande** :
```powershell
ollama run llama3.2 "Un produit co√ªte 120‚Ç¨. On applique une r√©duction de 20%, puis une taxe de 10% sur le prix r√©duit. Quel est le prix final ? Explique √©tape par √©tape."
```

**R√©sultat attendu** :
```
√âtape 1 : R√©duction = 120‚Ç¨ √ó 20% = 24‚Ç¨
√âtape 2 : Prix apr√®s r√©duction = 120‚Ç¨ - 24‚Ç¨ = 96‚Ç¨
√âtape 3 : Taxe = 96‚Ç¨ √ó 10% = 9.6‚Ç¨
√âtape 4 : Prix final = 96‚Ç¨ + 9.6‚Ç¨ = 105.6‚Ç¨

R√©ponse : 105,6‚Ç¨
```

---

## üß™ D√©monstration 3 : Raisonnement logique

### Probl√®me complexe

**Sans CoT** :
```powershell
ollama run llama3.2 "Si 5 machines fabriquent 5 widgets en 5 minutes, combien de temps faut-il √† 100 machines pour fabriquer 100 widgets ?"
```

**Pi√®ge** : R√©ponse intuitive incorrecte (100 minutes).

---

**Avec CoT** :
```powershell
ollama run llama3.2 "Si 5 machines fabriquent 5 widgets en 5 minutes, combien de temps faut-il √† 100 machines pour fabriquer 100 widgets ? Raisonne √©tape par √©tape."
```

**R√©sultat attendu** :
```
√âtape 1 : 5 machines ‚Üí 5 widgets en 5 minutes
√âtape 2 : Donc 1 machine ‚Üí 1 widget en 5 minutes
√âtape 3 : 100 machines travaillent en parall√®le
√âtape 4 : Chaque machine fait 1 widget en 5 minutes
√âtape 5 : 100 machines ‚Üí 100 widgets en 5 minutes

R√©ponse : 5 minutes
```

**‚úÖ Le CoT √©vite le pi√®ge !**

---

## üß™ D√©monstration 4 : Probl√®me de mots

**Commande avec CoT** :
```powershell
ollama run llama3.2 "Marie a le double de l'√¢ge de Jean. Dans 5 ans, la somme de leurs √¢ges sera 35 ans. Quel √¢ge ont-ils maintenant ? R√©sous √©tape par √©tape."
```

**Solution attendue** :
```
Soit x = √¢ge de Jean
Alors 2x = √¢ge de Marie

Dans 5 ans :
Jean aura x + 5 ans
Marie aura 2x + 5 ans

√âquation : (x + 5) + (2x + 5) = 35
           3x + 10 = 35
           3x = 25
           x = 8.33... 

V√©rification : Jean = 8.33 ans, Marie = 16.67 ans
Dans 5 ans : 13.33 + 21.67 = 35 ‚úì

R√©ponse : Jean a environ 8 ans, Marie environ 17 ans
```

---

## Partie 3 : Variantes du Chain of Thought

### Vue d'ensemble

| Technique | Description | Complexit√© |
|-----------|-------------|-------------|
| **Zero-Shot CoT** | Instruction simple ("Let's think step by step") | Faible |
| **Few-Shot CoT** | Exemples de raisonnements fournis avant la question | Moyenne |
| **Self-Consistency** | G√©n√©ration multiple + vote majoritaire | √âlev√©e |

---

### Exercice 5 : Few-Shot Chain of Thought

**Probl√®me** : "Un restaurant a 12 tables de 4 personnes et 8 tables de 6 personnes. Combien de clients maximum ?"

**√âtape 1** : Cr√©ez un exemple similaire

```powershell
ollama run llama3.2 "Exemple :
Q: Un caf√© a 5 tables de 3 personnes et 4 tables de 2 personnes. Combien de clients maximum ?
Raisonnement:
- 5 tables √ó 3 personnes = 15 personnes
- 4 tables √ó 2 personnes = 8 personnes
- Total = 15 + 8 = 23 personnes
R: 23 clients maximum

Maintenant r√©sous :
Q: Un restaurant a 12 tables de 4 personnes et 8 tables de 6 personnes. Combien de clients maximum ?"
```

**Observation** : Le mod√®le suit le format de l'exemple !

---

### Exercice : Self-Consistency manuelle

**Objectif** : G√©n√©rer 3 raisonnements et comparer

**Probl√®me** : "Si un livre a 450 pages et je lis 23 pages par jour, en combien de jours je finis le livre ?"

**Commande** (r√©p√©tez 3 fois) :
```powershell
ollama run llama3.2 "Si un livre a 450 pages et je lis 23 pages par jour, en combien de jours je finis le livre ? R√©sous √©tape par √©tape."
```

**Notez les r√©ponses** :
- G√©n√©ration 1 : ______ jours
- G√©n√©ration 2 : ______ jours
- G√©n√©ration 3 : ______ jours

**Vote majoritaire** : ______ jours

**Discussion** : Les r√©ponses sont-elles identiques ? Pourquoi des diff√©rences ?

---

### Exercice : Comparer fran√ßais vs anglais

**Hypoth√®se** : Le CoT fonctionne mieux en anglais (mod√®les entra√Æn√©s surtout en anglais)

**Test en fran√ßais** :
```powershell
ollama run llama3.2 "Un train part de Montr√©al √† 8h et roule √† 100 km/h. Un autre part de Qu√©bec (250 km) √† 9h √† 120 km/h vers Montr√©al. √Ä quelle heure se croisent-ils ? R√©sous √©tape par √©tape."
```

**Test en anglais** :
```powershell
ollama run llama3.2 "A train leaves Montreal at 8am traveling at 100 km/h. Another leaves Quebec City (250 km away) at 9am at 120 km/h toward Montreal. When do they meet? Let's solve this step by step."
```

**Comparez** :
- Pr√©cision : _____ (fran√ßais) vs _____ (anglais)
- Clart√© du raisonnement : _____ vs _____
- Temps de g√©n√©ration : _____ vs _____

---

## Partie 4 : Automatisation et mesure de performance

### Script d'automatisation Python

Ce script permet de comparer syst√©matiquement les performances avec et sans CoT :

```python
import subprocess
import time
import re

def query_ollama(prompt, model="llama3.2"):
    """Ex√©cute une requ√™te Ollama et retourne la r√©ponse"""
    result = subprocess.run(
        ["ollama", "run", model, prompt],
        capture_output=True,
        text=True,
        timeout=30
    )
    return result.stdout.strip()

def compare_cot(question, expected_answer, model="llama3.2"):
    """Compare les performances avec et sans CoT"""
    
    # Test sans CoT
    print(f"\nQuestion : {question}")
    print(f"R√©ponse attendue : {expected_answer}\n")
    
    print("--- SANS CHAIN OF THOUGHT ---")
    start = time.time()
    response_no_cot = query_ollama(question, model)
    time_no_cot = time.time() - start
    print(f"R√©ponse : {response_no_cot}")
    print(f"Temps : {time_no_cot:.2f}s")
    
    # Test avec CoT
    print("\n--- AVEC CHAIN OF THOUGHT ---")
    start = time.time()
    response_cot = query_ollama(f"{question} Let's solve this step by step.", model)
    time_cot = time.time() - start
    print(f"R√©ponse : {response_cot}")
    print(f"Temps : {time_cot:.2f}s")
    
    # Comparaison
    print(f"\nRatio temps : {time_cot/time_no_cot:.2f}x")
    print(f"Tokens estim√©s : ~{len(response_no_cot.split())} vs ~{len(response_cot.split())}")
    
    return {
        'no_cot': {'response': response_no_cot, 'time': time_no_cot},
        'cot': {'response': response_cot, 'time': time_cot}
    }

# Exemple d'utilisation
if __name__ == "__main__":
    problems = [
        {
            "question": "A store has 23 apples. It receives 45 in the morning and sells 18 in the afternoon. How many apples remain?",
            "answer": "50"
        },
        {
            "question": "A product costs 120 euros. A 20% discount is applied, then a 10% tax on the reduced price. What is the final price?",
            "answer": "105.6"
        },
        {
            "question": "If 5 machines make 5 widgets in 5 minutes, how long does it take 100 machines to make 100 widgets?",
            "answer": "5"
        }
    ]
    
    for i, problem in enumerate(problems, 1):
        print(f"\n{'='*60}")
        print(f"PROBL√àME {i}")
        print(f"{'='*60}")
        compare_cot(problem['question'], problem['answer'])
```

### Ex√©cution

```powershell
python test_cot.py
```

### R√©sultats attendus

| M√©trique | Sans CoT | Avec CoT | Variation |
|----------|----------|----------|--------|
| **Pr√©cision** | 40-60% | 70-85% | +25-40% |
| **Temps** | 2-4s | 4-8s | 2x |
| **Tokens** | ~50 | ~150 | 3x |
| **V√©rifiabilit√©** | Non | Oui | - |

---

## Corrig√© des exercices

### Exercice 1 : Probl√®me arithm√©tique

**R√©ponse attendue** : 50 pommes

**Raisonnement correct** :
```
√âtape 1 : Inventaire initial = 23 pommes
√âtape 2 : Apr√®s r√©ception = 23 + 45 = 68 pommes
√âtape 3 : Apr√®s vente = 68 - 18 = 50 pommes
R√©ponse : 50 pommes
```

### Exercice 2 : Calcul avec pourcentages

**R√©ponse attendue** : 105,6‚Ç¨

**Raisonnement correct** :
```
√âtape 1 : R√©duction = 120‚Ç¨ √ó 20% = 24‚Ç¨
√âtape 2 : Prix r√©duit = 120‚Ç¨ - 24‚Ç¨ = 96‚Ç¨
√âtape 3 : Taxe = 96‚Ç¨ √ó 10% = 9,6‚Ç¨
√âtape 4 : Prix final = 96‚Ç¨ + 9,6‚Ç¨ = 105,6‚Ç¨
```

### Exercice 3 : Pi√®ge logique

**R√©ponse attendue** : 5 minutes

**Raisonnement correct** :
```
√âtape 1 : 5 machines ‚Üí 5 widgets en 5 minutes
√âtape 2 : Donc 1 machine ‚Üí 1 widget en 5 minutes
√âtape 3 : 100 machines travaillent en parall√®le
√âtape 4 : Chaque machine produit 1 widget en 5 minutes
√âtape 5 : R√©sultat : 100 machines ‚Üí 100 widgets en 5 minutes
```

**Pi√®ge √©vit√©** : La r√©ponse intuitive (mais incorrecte) est 100 minutes.

### Exercice 4 : Probl√®me alg√©brique

**R√©ponse attendue** : Jean ‚âà 8,3 ans, Marie ‚âà 16,7 ans

**Raisonnement correct** :
```
Soit x = √¢ge de Jean
Alors 2x = √¢ge de Marie

Dans 5 ans :
(x + 5) + (2x + 5) = 35
3x + 10 = 35
3x = 25
x = 8,33 ans

V√©rification :
Jean = 8,33 ans, Marie = 16,67 ans
Dans 5 ans : 13,33 + 21,67 = 35 ‚úì
```

### Exercice 5 : Few-Shot CoT

**R√©ponse attendue** : 96 clients maximum

**Raisonnement correct** :
```
√âtape 1 : Tables de 4 personnes = 12 √ó 4 = 48 personnes
√âtape 2 : Tables de 6 personnes = 8 √ó 6 = 48 personnes  
√âtape 3 : Total = 48 + 48 = 96 clients maximum
```

### Exercice 6 : Self-Consistency

**R√©ponse attendue** : 20 jours (ou 19,57 jours si pr√©cis)

**Raisonnement correct** :
```
√âtape 1 : Pages totales = 450
√âtape 2 : Pages par jour = 23
√âtape 3 : Jours n√©cessaires = 450 √∑ 23 = 19,57 jours
√âtape 4 : Arrondi : 20 jours complets
```

---

## Synth√®se et observations

### Comparaison des techniques

| Crit√®re | Sans CoT | Zero-Shot CoT | Few-Shot CoT |
|---------|----------|---------------|-------------|
| **Pr√©cision** | 40-60% | 70-80% | 80-85% |
| **Temps** | Rapide | Moyen | Lent |
| **Co√ªt (tokens)** | Faible | Moyen | √âlev√© |
| **Contr√¥le** | Aucun | Faible | √âlev√© |
| **Complexit√©** | Minimale | Faible | Moyenne |

### Cas d'usage recommand√©s

**Utiliser le Chain of Thought pour** :
- Probl√®mes math√©matiques et scientifiques
- Raisonnement logique complexe
- Analyse de donn√©es
- D√©cisions critiques (finance, sant√©, l√©gal)
- Apprentissage et tutorat

**√âviter le Chain of Thought pour** :
- Questions factuelles simples
- G√©n√©ration cr√©ative
- Applications temps r√©el strictes
- Ressources limit√©es (m√©moire, calcul)

---

## R√©f√©rences

### Documentation technique
- **Ollama** : https://github.com/ollama/ollama
- **Mod√®les disponibles** : https://ollama.com/library

### Articles scientifiques
- Wei, J., et al. (2022). "Chain-of-Thought Prompting Elicits Reasoning in Large Language Models". *arXiv:2201.11903*
- Kojima, T., et al. (2022). "Large Language Models are Zero-Shot Reasoners". *arXiv:2205.11916*
- Wang, X., et al. (2022). "Self-Consistency Improves Chain of Thought Reasoning in Language Models". *arXiv:2203.11171*

---

## D√©pannage

### Probl√®me : Ollama ne r√©pond pas

**Solution** :
```powershell
# Red√©marrer le service
Stop-Process -Name "ollama" -Force
ollama serve
```

### Probl√®me : Mod√®le trop lent

**Solutions** :
1. Utiliser un mod√®le plus l√©ger : `llama3.2` au lieu de `llama3.1`
2. R√©duire la longueur du contexte
3. V√©rifier les ressources syst√®me (RAM, CPU)

### Probl√®me : R√©ponses incoh√©rentes

**Solutions** :
1. Baisser la temp√©rature : `--temperature 0.3`
2. Reformuler la question
3. Essayer un mod√®le diff√©rent

### Probl√®me : Erreur de m√©moire

**Solution** :
- Utiliser un mod√®le plus petit
- Fermer les applications inutilis√©es
- Augmenter la m√©moire virtuelle

### test_cot_ollama.py

```python
import subprocess
import time

def query_ollama(prompt, model="llama3.2"):
    """Envoie une requ√™te √† Ollama et retourne la r√©ponse"""
    result = subprocess.run(
        ["ollama", "run", model, prompt],
        capture_output=True,
        text=True,
        timeout=30
    )
    return result.stdout.strip()

def compare_cot():
    """Compare les r√©ponses avec et sans CoT"""
    
    problems = [
        {
            "question": "Un magasin a 23 pommes. Il re√ßoit 45 pommes le matin, puis vend 18 pommes l'apr√®s-midi. Combien reste-t-il de pommes ?",
            "answer": "50"
        },
        {
            "question": "Un produit co√ªte 120‚Ç¨. On applique une r√©duction de 20%, puis une taxe de 10% sur le prix r√©duit. Quel est le prix final ?",
            "answer": "105.6"
        },
        {
            "question": "Si 5 machines fabriquent 5 widgets en 5 minutes, combien de temps faut-il √† 100 machines pour fabriquer 100 widgets ?",
            "answer": "5"
        }
    ]
    
    for i, problem in enumerate(problems, 1):
        print(f"\n{'='*60}")
        print(f"PROBL√àME {i}")
        print(f"{'='*60}")
        print(f"\nQuestion : {problem['question']}")
        print(f"R√©ponse attendue : {problem['answer']}")
        
        # Sans CoT
        print(f"\n--- SANS CHAIN OF THOUGHT ---")
        start = time.time()
        response_no_cot = query_ollama(problem['question'])
        time_no_cot = time.time() - start
        print(f"R√©ponse : {response_no_cot}")
        print(f"Temps : {time_no_cot:.2f}s")
        
        # Avec CoT
        print(f"\n--- AVEC CHAIN OF THOUGHT ---")
        start = time.time()
        response_cot = query_ollama(f"{problem['question']} R√©sous √©tape par √©tape.")
        time_cot = time.time() - start
        print(f"R√©ponse : {response_cot}")
        print(f"Temps : {time_cot:.2f}s")
        
        print(f"\nRatio temps : {time_cot/time_no_cot:.2f}x plus lent avec CoT")

if __name__ == "__main__":
    print("üß™ TEST : Chain of Thought avec Ollama")
    print("Mod√®le : llama3.2")
    compare_cot()
```

**Lancer le script** :
```powershell
python test_cot_ollama.py
```

---

## üìä Observations attendues

### R√©sultats typiques

| M√©trique | Sans CoT | Avec CoT | Am√©lioration |
|----------|----------|----------|--------------|
| **Pr√©cision** | 40-60% | 70-85% | **+25-30%** |
| **Temps** | 2-4s | 4-8s | **2x plus lent** |
| **Tokens** | ~50 | ~150 | **3x plus** |
| **V√©rifiabilit√©** | ‚ùå Non | ‚úÖ Oui | - |

### Points cl√©s observ√©s

1. **Transparence** : Le CoT montre le raisonnement
2. **Erreurs d√©tectables** : On voit o√π le mod√®le se trompe
3. **Co√ªt** : 2-3x plus de temps de calcul
4. **Performance** : Meilleure sur probl√®mes complexes

---

## üéØ Variantes √† tester

### 1. Zero-Shot CoT (simple)

```powershell
ollama run llama3.2 "[question] Let's think step by step."
```

### 2. Few-Shot CoT (avec exemple)

```powershell
ollama run llama3.2 "Exemple :
Q: Jean a 3 bo√Ætes de 12 crayons. Combien en tout ?
Raisonnement:
- Jean a 3 bo√Ætes
- Chaque bo√Æte = 12 crayons
- Total = 3 √ó 12 = 36 crayons
R: 36 crayons

Maintenant r√©sous :
Q: Marie a 4 paquets de 15 bonbons. Combien en tout ?"
```

### 3. CoT en fran√ßais vs anglais

**Tester** : Le CoT fonctionne-t-il mieux en anglais ?

```powershell
# En fran√ßais
ollama run llama3.2 "[question] R√©sous √©tape par √©tape."

# En anglais
ollama run llama3.2 "[question in English] Let's solve this step by step."
```

---

## üí° Instructions CoT efficaces

**Variantes test√©es** :

1. "R√©sous √©tape par √©tape"
2. "Explique ton raisonnement"
3. "Montre tes calculs interm√©diaires"
4. "D√©compose le probl√®me"
5. "Let's think step by step" (anglais)
6. "Let's approach this methodically" (anglais)

**La plus efficace** : "Let's think step by step" (en anglais)

---

## üîß D√©pannage

### Probl√®me : Ollama ne r√©pond pas

**Solution** :
```powershell
# Red√©marrer le service
Stop-Process -Name "ollama" -Force
ollama serve
```

### Probl√®me : Mod√®le trop lent

**Solutions** :
1. Utiliser un mod√®le plus petit : `ollama pull llama3.2`
2. R√©duire le contexte
3. Limiter la longueur de r√©ponse

### Probl√®me : R√©ponses incoh√©rentes

**Solutions** :
1. Baisser la temp√©rature : `ollama run llama3.2 --temperature 0.3`
2. Reformuler la question
3. Essayer un autre mod√®le : `mistral` ou `phi3`

---

## üéì Questions pour la discussion

1. **Pourquoi le CoT am√©liore-t-il les performances ?**
   - D√©composition du probl√®me
   - M√©moire de travail √©tendue
   - Auto-correction possible

2. **Quand NE PAS utiliser le CoT ?**
   - Questions factuelles simples
   - Applications temps r√©el
   - Ressources limit√©es

3. **Le CoT fonctionne-t-il sur tous les mod√®les ?**
   - Non, meilleur sur grands mod√®les (>3B param√®tres)
   - Performance variable selon l'architecture

4. **Peut-on combiner CoT avec d'autres techniques ?**
   - Oui : RAG, Self-Consistency, Tree of Thoughts

---

## üìö Ressources compl√©mentaires

**Pour aller plus loin** :
- Documentation Ollama : https://github.com/ollama/ollama
- Mod√®les disponibles : https://ollama.com/library
- Paper original CoT : https://arxiv.org/abs/2201.11903

**Mod√®les recommand√©s pour CoT** :
- `llama3.1` (8B) - Excellent raisonnement
- `mistral` (7B) - Bon √©quilibre
- `phi3` (3.8B) - Rapide et efficace


